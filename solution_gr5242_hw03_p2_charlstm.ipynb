{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **GR5242 HW03 Problem 2: Shakespeare with LSTM networks**\n","\n","**Instructions**: This problem is an individual assignment -- you are to complete this problem on your own, without conferring with your classmates.  You should submit a completed and published notebook to Courseworks; no other files will be accepted.\n","\n","## Description:\n","\n","This homework exercise has 3 primary goals:\n"," * Introduce some basic concepts from natural language processing\n"," * Get some practice training recurrent neural networks, specifically on text data\n"," * Be able to generate fake text data from your favorite author!   \n","\n","By the end of this exercise, you will have a basic, but decent, computer program which can simulate the writing patterns of any author of your choice.\n","\n","Here is an outline of the rest of the exercise.\n"," 1. Data loading\n","     - We will start by downloading a text from Project Gutenberg that we will try to model\n","     - Data preprocessing and numerical encoding\n","     - Making a training `Dataset` object\n"," 3. Learn to generate text with a neural network\n","     - Defining the recurrent network\n","     - Training\n","     - Predicting and sampling text from the model\n","\n","     There are 12 questions (70 points) in total, which include coding and written questions. You can only modify the codes and text within \\### YOUR CODE HERE ### and/or \\### YOUR ANSWER HERE ###.\n"],"metadata":{"id":"h5vtG0bbJt9u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hwabXFdkMlH"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import re"]},{"cell_type":"markdown","source":["## Character-level language modeling\n","\n","Our goal here is to build a model of language letter-by-letter. Since we may also allow numbers, spaces, and punctuation, it's better to say character-by-character. We will start by fixing an \"alphabet\": the set of allowed characters.\n","\n","In math notation, let's call the alphabet $A$. In code,"],"metadata":{"id":"6_-QkOsDQrgu"}},{"cell_type":"code","source":["alphabet = \" abcdefghijklmnopqrstuvwxyz1234567890.,!?:;ABCDEFGHIJKLMNOPQRSTUVWXYZ\\n\""],"metadata":{"id":"K8RdHuHPRCPJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 1: Data loading and preprocessing"],"metadata":{"id":"sitlEu4BJsBm"}},{"cell_type":"markdown","source":["We will start by downloading training data from Project Gutenberg: https://www.gutenberg.org/. Project Gutenberg is a free repository of public domain books. Find any book you like, and download it in Plain Text UTF-8 format.\n","\n","For example, we will use Shakespeare's complete works: https://www.gutenberg.org/ebooks/100. There is a link on that page to the Plain Text format data.  Download the pg100.txt file, and then upload it from your computer to colab (click at left on the File icon, then click the upload icon).  \n","\n","*Important*: whichever work you choose, make sure you have enough data! The size of your plain text file should be at least 2MB."],"metadata":{"id":"Oryq1WYLNojq"}},{"cell_type":"code","source":["# after uploading your file to Colab, set this variable to its path:\n","txt_path = \"pg100.txt\""],"metadata":{"id":"NeGF0qSbJqqB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's load the text and see what it says:"],"metadata":{"id":"GGNYDQKRSXBk"}},{"cell_type":"code","source":["with open(txt_path) as txt_file:\n","  text = txt_file.read()\n","\n","print(\"text is\", len(text), \"characters long.\")\n","print()\n","print(\"A sample from the middle:\")\n","print()\n","print(text[len(text) // 2 : len(text) // 2 + 100])"],"metadata":{"id":"7RKK_A1NShRW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data standardization\n","\n","Now, we will clean the data: converting the data to lowercase, removing extra spaces and linebreaks, and get rid of characters which are not in our alphabet."],"metadata":{"id":"7viKRe94P_Cx"}},{"cell_type":"code","source":["# remove extra characters by replacing them with spaces\n","text = re.sub(rf\"[^{alphabet}]\", \" \", text)"],"metadata":{"id":"5m3ZqU7RS2xA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how it looks again:"],"metadata":{"id":"7tgMpHS2O67G"}},{"cell_type":"code","source":["print(text[len(text) // 2 : len(text) // 2 + 100])\n","x_prompt = text[len(text) // 2 : len(text) // 2 + 1000]"],"metadata":{"id":"TiJEL-O-O6wL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Numerical encoding\n","\n","Unfortunately, neural networks don't understand text. So, we need to convert our characters to numerical values. Here are some helper functions for doing this."],"metadata":{"id":"8JFrb5Jr92F8"}},{"cell_type":"code","source":["# let's build a dictionary mapping characters to integers\n","char2int = {c: i for i, c in enumerate(alphabet)}\n","alphabet_array = np.array([c for c in alphabet])\n","\n","# this function will turn a string into a numpy array of integers\n","def int_encode(string):\n","  if any(c not in char2int for c in string):\n","    raise ValueError(\n","        \"Found a character which was not in the alphabet in the input \"\n","        f\"to int_encode. Valid alphabet characters: {alphabet}\"\n","      )\n","  return np.array([char2int[c] for c in string])\n","\n","# this function will decode a numpy array of integers back to a string\n","def int_decode(int_array):\n","  return ''.join(alphabet_array[int_array])"],"metadata":{"id":"1LjvsaNy91to"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Question 1a: 4 points) Test out `int_encode` by passing `test_string` in and printing the result."],"metadata":{"id":"LsSPJalSWFUm"}},{"cell_type":"code","source":["# Let's test these out!\n","### YOUR CODE HERE ### "],"metadata":{"id":"vU8Umqsw-4CN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ANSWER ### \n","test_string = \"hi, How do these characters encode?\"\n","test_encoding = int_encode(test_string)\n","test_encoding"],"metadata":{"id":"8bSlzTIKz_O_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Question 1b: 4 points) Decode the result from the last cell using `int_decode` to make sure it is the same as `test_string`"],"metadata":{"id":"wGFYXZPcWL8Z"}},{"cell_type":"code","source":["### YOUR CODE HERE ###"],"metadata":{"id":"392dWPKn0J85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ANSWER ###\n","int_decode(test_encoding)"],"metadata":{"id":"cmJOEkQk_aIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Is the decoding the same as `test_string`? It should -- you have a bug above if not."],"metadata":{"id":"SWxJykne_VKY"}},{"cell_type":"markdown","source":["### Make a training dataset\n","\n","First, we make a numerical encoded version of the entire dataset:"],"metadata":{"id":"oIqRJLtyAC--"}},{"cell_type":"code","source":["enctext = int_encode(text)"],"metadata":{"id":"JOr_T3ddALvQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use `tf.convert_to_tensor` to make it into a TensorFlow tensor:"],"metadata":{"id":"j1FLc9MABXKu"}},{"cell_type":"code","source":["enctext = tf.convert_to_tensor(enctext)"],"metadata":{"id":"ORDJL4_0BWTK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, convert to a tensorflow `Dataset` using `tf.data.Dataset.from_tensor_slices`:"],"metadata":{"id":"VDmg1fVsBALW"}},{"cell_type":"code","source":["text_dataset = tf.data.Dataset.from_tensor_slices(enctext)\n","text_dataset"],"metadata":{"id":"rXIvdUjaAPVc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 2: Training a NN\n","\n","Our model will work as follows:\n"," - One-hot encoded input gets passed into a linear embedding layer. These two operations are combined with the `Embedding` layer: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n"," - LSTM cell\n"," - Linear decoder layer\n","\n","TensorFlow/Keras has two main ways of interfacing with recurrent networks. In the case of LSTMs, those are:\n"," - the LSTM layer https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n"," - the LSTMCell layer https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell\n","\n","Both models are sequential: the goal is to process a batch of sequences of input features and produce a batch of sequences of output features. The `LSTM` class makes this simple and easy, and the `LSTMCell` class gives more control by allowing you to process the sequences one element at a time. We will use the `LSTM` layer to keep things simple, but keep in mind that some of what we do could be made more efficient with `LSTMCell`.\n","\n","The inputs and outputs to recurrent networks in Keras have shape: `(batch_dimension, sequence_dimension, feature_dimension)`. In this case, our feature dimension is `len(alphabet)`.\n","\n","Something to keep in mind: the output of this network will be stateful! In each batch, the `k`th output along the sequence dimension will be the logits for predicting the `k+1`th input in the batch."],"metadata":{"id":"Pk3z0bACJyQc"}},{"cell_type":"code","source":["# We will use this constant below\n","HIDDEN_DIM = 128"],"metadata":{"id":"F80hsi-FH4WA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Question 2a: 10 points) Model definition: make a Sequential model with an Embedding layer with input dimension `len(alphabet)` and output dimension `HIDDEN_DIM`, followed by an LSTM layer with `HIDDEN_DIM` features, followed by a Dense layer with `len(alphabet)` features"],"metadata":{"id":"d2syDhfkWdXt"}},{"cell_type":"code","source":["model = ... # YOUR CODE HERE"],"metadata":{"id":"REnQC0MXiyAL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ANSWER ###\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(len(alphabet), HIDDEN_DIM),\n","    tf.keras.layers.LSTM(HIDDEN_DIM, return_sequences=True),\n","    tf.keras.layers.Dense(len(alphabet)),\n","])"],"metadata":{"id":"3O4jvGb4EmxE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Question 2b: 8 points) If we want to use the output of the model as logits for predicting a character (which we can think of as a class), what loss should we use?"],"metadata":{"id":"fzp4Q7vFJXVj"}},{"cell_type":"code","source":["loss = ... # YOUR CODE HERE"],"metadata":{"id":"u1XFATDzieB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ANSWER ###\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"],"metadata":{"id":"Y-5sgPZkIsI2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=loss)"],"metadata":{"id":"RsZoPv7wUilY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining some parameters about data batching, explained in the next section\n","# Note: after you get the entire assignment working, you can make these bigger and train for longer, to get better performance\n","SEQUENCE_LENGTH = 32\n","BATCH_SIZE = 16"],"metadata":{"id":"_ZUt-1--LP32"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Making the dataset of (input, target) pairs\n","\n","To train the model, we need to make a `tf.data.Dataset` containing input and target sequences. Our input sequences will be sequences of length `SEQUENCE_LENGTH` containing int-encoded characters from the input. Our target sequences will be the \"next characters\" corresponding to the input sequence: so, if the input sequence is the 10th, 11th, ... characters, then the target sequence is the 11th, 12th, ... characters.\n","\n","We will walk through using `tf.data.Dataset` methods to create these."],"metadata":{"id":"h2ORoPzNJkKP"}},{"cell_type":"markdown","source":["(Question 2c: 8 points) Use our friend the `batch` method of `text_dataset`, which we defined above, to make sequences of length `SEQUENCE_LENGTH`."],"metadata":{"id":"7PlSDJOjL_Fb"}},{"cell_type":"code","source":["input_seqs = ... # YOUR CODE HERE\n","input_seqs"],"metadata":{"id":"dX6fJc84W7Lo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ANSWER ###\n","input_seqs = text_dataset.batch(SEQUENCE_LENGTH)\n","input_seqs"],"metadata":{"id":"oucBXNGzJ1U9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Question 2d: 8 points) Now, use batch again to create target sequences from the following version of the dataset which has been offset by 1 element:"],"metadata":{"id":"n24Q5ySjU2ed"}},{"cell_type":"code","source":["target_text_dataset = text_dataset.skip(1)\n","target_seqs = ... # YOUR CODE HERE"],"metadata":{"id":"BkjMZfS3XAae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ANSWER ###\n","target_text_dataset = text_dataset.skip(1)\n","target_seqs = target_text_dataset.batch(SEQUENCE_LENGTH)"],"metadata":{"id":"Nu63T8f4SdYJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Question 2e: 6 points) Now, use the function `tf.data.Dataset.zip` to create a dataset of (input, target) pairs:"],"metadata":{"id":"geD7pQ7TVH1Z"}},{"cell_type":"code","source":["pairs = ... # YOUR CODE HERE\n","pairs"],"metadata":{"id":"B5rrw7OLXGjS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SOLUTION\n","pairs = tf.data.Dataset.zip((input_seqs, target_seqs))\n","pairs"],"metadata":{"id":"NW9bGOdGUzmp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Question 2f: 4 points) Finally, call `.batch` again to make batches of pairs of length `BATCH_SIZE`:"],"metadata":{"id":"JX1_4b0oVtdv"}},{"cell_type":"code","source":["pairs_batched = ... # YOUR CODE HERE\n","pairs_batched"],"metadata":{"id":"pE547bYpXJge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ANSWER ###\n","pairs_batched = pairs.batch(BATCH_SIZE)\n","pairs_batched"],"metadata":{"id":"VRI2U9BPTP4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This fixes the size of the training data\n","# 8000 batches is a reasonable starting number.\n","train = pairs_batched.shuffle(1000).take(8000) "],"metadata":{"id":"7IXwT0d9cSJW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Question 2g: 2 points) Train the model! \n","Note:\n","\n","\n","1.   Given the above parameters, this training should take about 5 minutes.  \n","2.   Performance will improve but will only be mediocre.  For this exercise, that is adequate.  \n","3. However, to really see what this model can do, you can (should!) set a larger number of batches above and a larger hidden state so that you can take many epochs of the data.  If you train for ~10hrs on a model with 256 hidden states (as we show in class), performance can be quite good.  This step is not required for the assignment, but please do try it on your own.\n","\n"],"metadata":{"id":"Psw5Vbpk3J3Q"}},{"cell_type":"code","source":["### YOUR CODE HERE ###"],"metadata":{"id":"wrOmFS_eV21G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ANSWER ###\n","model.fit(train)"],"metadata":{"id":"JKD_eBNq1P-_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, make sure the loss goes down as it trains."],"metadata":{"id":"1WbYcgKmj1zL"}},{"cell_type":"markdown","source":["# Section 3: Did it work? Let's see what the model learned\n","\n","Here, we'll write some functions to see how well the model has learned to predict text and to draw samples from the model.\n","\n","First, we'll give you a function to \"seed\" the model with some input text and then predict the most likely future text. It will be your job to create a variation on this function in the question below, so make sure you understand how it works."],"metadata":{"id":"rmysazCNc14W"}},{"cell_type":"code","source":["def predict(seed_string, sample_length=50):\n","  # Convert seed_string to int\n","  current_text_ints = list(int_encode(seed_string))\n","\n","  for i in range(sample_length):\n","    # Add an empty batch dimension and convert to tensor\n","    text_arr = np.array(current_text_ints).reshape(1, -1)\n","    text_arr = tf.convert_to_tensor(text_arr)\n","\n","    # Get the full sequence of predictions, remove the batch dim\n","    logits = model(text_arr)[0]\n","\n","    # Remove the batch dimension and get the final logits\n","    final_logits = logits[-1]\n","\n","    # Get the prediction using tf.argmax\n","    pred = tf.argmax(final_logits)\n","\n","    # Append this to `current_text_ints`\n","    current_text_ints.append(pred.numpy())\n","  \n","  return int_decode(np.array(current_text_ints))"],"metadata":{"id":"UUaB5I6VXTdi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_seed = \"to be, or \"\n","predict(test_seed, 50)"],"metadata":{"id":"90ht1OAtegjv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# feel free to try your own seed!"],"metadata":{"id":"1gT8Oi_KgocR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It seems like maybe the model learned something, but the output is a little boring. Let's make it more interesting with *randomness*!\n","\n","Right now, the function always picks the most likely next letter. Instead, let's sample the next letter from the model's predicted probability distribution.\n","\n","(Question 3a: 8 points) Fill in the blanks in the function below."],"metadata":{"id":"c5f92OC-gORk"}},{"cell_type":"code","source":["def generate(seed_string, sample_length=50):\n","  # Convert seed_string to int\n","  current_text_ints = list(int_encode(seed_string))\n","\n","  for i in range(sample_length):\n","    # Add an empty batch dimension and convert to tensor\n","    text_arr = np.array(current_text_ints).reshape(1, -1)\n","    text_arr = tf.convert_to_tensor(text_arr)\n","\n","    # Get the full sequence of predictions, remove the batch dim\n","    logits = model(text_arr)[0]\n","\n","    # Remove the batch dimension and get the final logits\n","    final_logits = logits[-1]\n","\n","    # Normalize the final_logits to a probability distribution\n","    probs = ...  # YOUR CODE HERE\n","\n","    # Call .numpy so we can use a numpy function\n","    probs = probs.numpy()\n","\n","    # Sample from the probability distribution using\n","    # the function np.random.choice\n","    sample = ...  # YOUR CODE HERE\n","\n","    # Append this to `current_text_ints`\n","    current_text_ints.append(sample)\n","  \n","  return int_decode(np.array(current_text_ints))"],"metadata":{"id":"m4Oo888KiQyS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SOLUTION\n","def generate(seed_string, sample_length=50):\n","  # Convert seed_string to int\n","  current_text_ints = list(int_encode(seed_string))\n","\n","  for i in range(sample_length):\n","    # Add an empty batch dimension and convert to tensor\n","    text_arr = np.array(current_text_ints).reshape(1, -1)\n","    text_arr = tf.convert_to_tensor(text_arr)\n","\n","    # Get the full sequence of predictions, remove the batch dim\n","    logits = model(text_arr)[0]\n","\n","    # Remove the batch dimension and get the final logits\n","    final_logits = logits[-1]\n","\n","    # Normalize the final_logits to a probability distribution\n","    probs = tf.nn.softmax(final_logits)\n","\n","    # Call .numpy so we can use a numpy function\n","    probs = probs.numpy()\n","\n","    # Sample from the probability distribution using\n","    # the function np.random.choice\n","    sample = np.random.choice(len(alphabet), p=probs)\n","\n","    # Append this to `current_text_ints`\n","    current_text_ints.append(sample)\n","  \n","  return int_decode(np.array(current_text_ints))"],"metadata":{"id":"tgXB1zOneh0N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Question 3b: 6 points) Test this function `generate`. Is its output different from `predict` when given the same seed? How does it differ, and why?"],"metadata":{"id":"Kp07ZVlUhtpy"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","print(text[len(text) // 2 : len(text) // 2 + 500])"],"metadata":{"id":"d75C0gswiOU2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SOLUTION\n","test_seed = \"to be, or\"\n","test_seed = text[len(text) // 2 : len(text) // 2 + 100]\n","print(generate(test_seed, 50))\n","# its output is different from predict, because it randomly samples the\n","# sequence rather than always taking the most likely guess"],"metadata":{"id":"5OyFQTwRhOLB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Question 3c: 2 point) Try running `generate` a few times with the same seed. Are the results the same or different? Why?"],"metadata":{"id":"Fy3J9i71hnWT"}},{"cell_type":"code","source":["# SOLUTION: they will be different, because generate is random.\n","print(generate(\"alas, poor yorick\"))\n","print(generate(\"when i a fat and bean fed horse beguile,\"))"],"metadata":{"id":"FS3OW29PhQP2"},"execution_count":null,"outputs":[]}]}